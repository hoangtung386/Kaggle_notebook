{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 52254,
          "databundleVersionId": 9674523,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "name": "Segment_RSNA_2D",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangtung386/Kaggle_notebook/blob/main/Segment_RSNA_2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "A3ZrwTdVPD9c"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "rsna_2023_abdominal_trauma_detection_path = kagglehub.competition_download('rsna-2023-abdominal-trauma-detection')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "1LMBp_YwPD9f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "imtyH7pDR7tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q monai wandb nibabel torchsummary\n",
        "!pip install -q protobuf==3.20.3"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:05:28.456154Z",
          "iopub.execute_input": "2025-11-20T19:05:28.456739Z",
          "iopub.status.idle": "2025-11-20T19:06:50.961245Z",
          "shell.execute_reply.started": "2025-11-20T19:05:28.456714Z",
          "shell.execute_reply": "2025-11-20T19:06:50.960413Z"
        },
        "id": "MCwZEx6bR7tM",
        "outputId": "1db67d01-2470-4741-ce9d-74cae306ac13"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from monai.transforms import (Compose, LoadImaged, EnsureChannelFirstd, Spacingd, ScaleIntensityRanged,\n",
        "                              Orientationd, CropForegroundd, SpatialPadd, RandAffined, ToTensord)\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.data import CacheDataset, list_data_collate\n",
        "from monai.utils import set_determinism\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import directed_hausdorff\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "import nibabel as nib"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:06:50.962557Z",
          "iopub.execute_input": "2025-11-20T19:06:50.962833Z",
          "iopub.status.idle": "2025-11-20T19:07:29.114826Z",
          "shell.execute_reply.started": "2025-11-20T19:06:50.962805Z",
          "shell.execute_reply": "2025-11-20T19:07:29.114174Z"
        },
        "id": "iqbRkEmDR7tN",
        "outputId": "7c2d54d3-f0c6-407b-9b27-4d3d2d28e892"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-11-20 19:07:08.065420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763665628.227135      89 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763665628.276872      89 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W&B Authentication"
      ],
      "metadata": {
        "id": "w3R2ZTdaR7tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "use_wandb = True  # Đặt thành False nếu không muốn dùng wandb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:07:29.115552Z",
          "iopub.execute_input": "2025-11-20T19:07:29.115819Z",
          "iopub.status.idle": "2025-11-20T19:07:29.124058Z",
          "shell.execute_reply.started": "2025-11-20T19:07:29.115796Z",
          "shell.execute_reply": "2025-11-20T19:07:29.123355Z"
        },
        "id": "L6smhyNVR7tP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if use_wandb:\n",
        "    try:\n",
        "        wandb_api = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_api)\n",
        "        print(\"W&B authentication successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"W&B authentication failed: {e}\")\n",
        "        print(\"Continuing without W&B logging\")\n",
        "        use_wandb = False"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:07:29.12573Z",
          "iopub.execute_input": "2025-11-20T19:07:29.125984Z",
          "iopub.status.idle": "2025-11-20T19:07:35.241703Z",
          "shell.execute_reply.started": "2025-11-20T19:07:29.125966Z",
          "shell.execute_reply": "2025-11-20T19:07:35.24114Z"
        },
        "id": "jKHOufhnR7tQ",
        "outputId": "1e769aa7-af25-443e-c39d-21ce72686e23"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhoangtung386\u001b[0m (\u001b[33mlevuhoangtung\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "W&B authentication successful!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "random_seed = 42\n",
        "set_determinism(seed=random_seed)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:07:35.242397Z",
          "iopub.execute_input": "2025-11-20T19:07:35.243074Z",
          "iopub.status.idle": "2025-11-20T19:07:35.250592Z",
          "shell.execute_reply.started": "2025-11-20T19:07:35.243053Z",
          "shell.execute_reply": "2025-11-20T19:07:35.250102Z"
        },
        "id": "uxdTNEhqR7tQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:07:35.251186Z",
          "iopub.execute_input": "2025-11-20T19:07:35.251394Z",
          "iopub.status.idle": "2025-11-20T19:07:35.288253Z",
          "shell.execute_reply.started": "2025-11-20T19:07:35.251378Z",
          "shell.execute_reply": "2025-11-20T19:07:35.287638Z"
        },
        "id": "9os43GE0R7tR",
        "outputId": "c94d3ad4-35c7-470b-ee72-ae2cc7ef236d"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I . Data preprocessing"
      ],
      "metadata": {
        "id": "DRq3FIWrR7tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Paths và Config"
      ],
      "metadata": {
        "id": "aoYLz6ONR7tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/kaggle/input/rsna-2023-abdominal-trauma-detection/'\n",
        "output_dir = '/kaggle/working'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "CONFIG = {\n",
        "    'seed': random_seed,\n",
        "    'train_split': 0.8,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs': 60,\n",
        "    'learning_rate': 1e-3,\n",
        "    'spatial_size': (256, 256),\n",
        "    'init_features': 32,\n",
        "    'num_classes': 6,\n",
        "    'cache_rate': 0,\n",
        "    'num_workers': 2,\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:07:35.28894Z",
          "iopub.execute_input": "2025-11-20T19:07:35.289109Z",
          "iopub.status.idle": "2025-11-20T19:07:35.301338Z",
          "shell.execute_reply.started": "2025-11-20T19:07:35.289095Z",
          "shell.execute_reply": "2025-11-20T19:07:35.300689Z"
        },
        "id": "OFTlGaQJR7tS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Data and Extract 2D Slices"
      ],
      "metadata": {
        "id": "fw4kTc4aR7tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "series_meta = pd.read_csv(os.path.join(base_path, 'train_series_meta.csv'))\n",
        "labels = pd.read_csv(os.path.join(base_path, 'train_2024.csv'))\n",
        "\n",
        "seg_path = os.path.join(base_path, 'segmentations')\n",
        "seg_files = [f for f in os.listdir(seg_path) if f.endswith('.nii')]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:07:35.30201Z",
          "iopub.execute_input": "2025-11-20T19:07:35.302219Z",
          "iopub.status.idle": "2025-11-20T19:07:35.3437Z",
          "shell.execute_reply.started": "2025-11-20T19:07:35.302203Z",
          "shell.execute_reply": "2025-11-20T19:07:35.343025Z"
        },
        "id": "l2Lv5nzMR7tS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting 2D slices from 3D volumes\")\n",
        "data_list = []\n",
        "\n",
        "for seg_file in tqdm(seg_files, desc=\"Processing volumes\"):\n",
        "    series_id = int(seg_file.split('.')[0])\n",
        "    patient_row = series_meta[series_meta['series_id'] == series_id]\n",
        "\n",
        "    if not patient_row.empty:\n",
        "        patient_id = int(patient_row['patient_id'].values[0])\n",
        "        image_dir = os.path.join(base_path, f'train_images/{patient_id}/{series_id}')\n",
        "        seg_file_path = os.path.join(seg_path, seg_file)\n",
        "\n",
        "        # Load segmentation to get number of slices\n",
        "        try:\n",
        "            seg_nii = nib.load(seg_file_path)\n",
        "            seg_data = seg_nii.get_fdata()\n",
        "\n",
        "            # Get slices that contain organs (not all background)\n",
        "            num_slices = seg_data.shape[2]\n",
        "            for slice_idx in range(num_slices):\n",
        "                slice_seg = seg_data[:, :, slice_idx]\n",
        "                # Only include slices with at least some organ segmentation\n",
        "                if np.sum(slice_seg > 0) > 100:  # At least 100 pixels with organs\n",
        "                    data_list.append({\n",
        "                        'image': image_dir,\n",
        "                        'seg': seg_file_path,\n",
        "                        'slice_idx': slice_idx\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {seg_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "print(f\"Total 2D slices extracted: {len(data_list)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T19:07:35.344409Z",
          "iopub.execute_input": "2025-11-20T19:07:35.344913Z"
        },
        "id": "Y_F0ynDwR7tS",
        "outputId": "7ccdad08-fa84-4f4f-eaf9-75d86ec677b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Extracting 2D slices from 3D volumes\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Processing volumes:  18%|█▊        | 37/206 [01:10<04:51,  1.73s/it]",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_list, val_list = train_test_split(\n",
        "    data_list,\n",
        "    test_size=1-CONFIG['train_split'],\n",
        "    random_state=CONFIG['seed'],\n",
        "    shuffle=True\n",
        ")\n",
        "print(f\"Train slices: {len(train_list)}, Val slices: {len(val_list)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "TCuh2qPlR7tS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Custom 2D Dataset"
      ],
      "metadata": {
        "id": "ftRMX1YSR7tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RSNA2DDataset(Dataset):\n",
        "    def __init__(self, data_list, transforms=None, spatial_size=(256, 256)):\n",
        "        self.data_list = data_list\n",
        "        self.transforms = transforms\n",
        "        self.spatial_size = spatial_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_dict = self.data_list[idx]\n",
        "        image_dir = data_dict['image']\n",
        "        seg_path = data_dict['seg']\n",
        "        slice_idx = data_dict['slice_idx']\n",
        "\n",
        "        # Load 3D volumes\n",
        "        from monai.transforms import LoadImage\n",
        "        loader = LoadImage(image_only=True)\n",
        "\n",
        "        # Load image (DICOM series)\n",
        "        image_3d = loader(image_dir)\n",
        "        # Load segmentation\n",
        "        seg_3d = loader(seg_path)\n",
        "\n",
        "        # Convert to numpy if torch tensor\n",
        "        if isinstance(image_3d, torch.Tensor):\n",
        "            image_3d = image_3d.numpy()\n",
        "        if isinstance(seg_3d, torch.Tensor):\n",
        "            seg_3d = seg_3d.numpy()\n",
        "\n",
        "        # Extract 2D slice\n",
        "        if len(image_3d.shape) == 4:\n",
        "            image_2d = image_3d[0, :, :, slice_idx]\n",
        "        else:\n",
        "            image_2d = image_3d[:, :, slice_idx]\n",
        "\n",
        "        if len(seg_3d.shape) == 4:\n",
        "            seg_2d = seg_3d[0, :, :, slice_idx]\n",
        "        else:\n",
        "            seg_2d = seg_3d[:, :, slice_idx]\n",
        "\n",
        "        # Ensure numpy array\n",
        "        image_2d = np.array(image_2d, dtype=np.float32)\n",
        "        seg_2d = np.array(seg_2d, dtype=np.float32)\n",
        "\n",
        "        # Ensure 2D shape (H, W)\n",
        "        if len(image_2d.shape) == 3:\n",
        "            image_2d = image_2d[0]\n",
        "        if len(seg_2d.shape) == 3:\n",
        "            seg_2d = seg_2d[0]\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transforms:\n",
        "            # Intensity scaling\n",
        "            image_2d = np.clip(image_2d, -125, 275)\n",
        "            image_2d = (image_2d + 125) / 400.0  # Normalize to [0, 1]\n",
        "\n",
        "            # Crop foreground\n",
        "            mask = image_2d > 0.1\n",
        "            if mask.sum() > 0:\n",
        "                coords = np.argwhere(mask)\n",
        "                y_min, x_min = coords.min(axis=0)\n",
        "                y_max, x_max = coords.max(axis=0)\n",
        "\n",
        "                # Add padding\n",
        "                pad = 10\n",
        "                y_min = max(0, y_min - pad)\n",
        "                x_min = max(0, x_min - pad)\n",
        "                y_max = min(image_2d.shape[0], y_max + pad)\n",
        "                x_max = min(image_2d.shape[1], x_max + pad)\n",
        "\n",
        "                image_2d = image_2d[y_min:y_max, x_min:x_max]\n",
        "                seg_2d = seg_2d[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            # Resize to fixed size\n",
        "            from skimage.transform import resize\n",
        "            image_2d = resize(image_2d, self.spatial_size, order=1, preserve_range=True)\n",
        "            seg_2d = resize(seg_2d, self.spatial_size, order=0, preserve_range=True)\n",
        "\n",
        "        # Add channel dimension (C, H, W)\n",
        "        image_2d = image_2d[np.newaxis, ...].astype(np.float32)\n",
        "        seg_2d = seg_2d[np.newaxis, ...].astype(np.int64)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        image_2d = torch.from_numpy(image_2d)\n",
        "        seg_2d = torch.from_numpy(seg_2d)\n",
        "\n",
        "        return {'image': image_2d, 'seg': seg_2d}"
      ],
      "metadata": {
        "trusted": true,
        "id": "cqIZJd_kR7tT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "C_NChiUdR7tT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data processing"
      ],
      "metadata": {
        "id": "YDkWADoWR7tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = RSNA2DDataset(train_list, transforms=True, spatial_size=CONFIG['spatial_size'])\n",
        "val_ds = RSNA2DDataset(val_list, transforms=True, spatial_size=CONFIG['spatial_size'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "2wGUrlXWR7tT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. DataLoader"
      ],
      "metadata": {
        "id": "tYznGbs3R7tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "0olW1pZVR7tT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Unet 2D Architecture"
      ],
      "metadata": {
        "id": "0dP-xzDWR7tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initialize the architecture"
      ],
      "metadata": {
        "id": "l8Q7sc05R7tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rSFqqc2LR7tU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cYoN1SAhR7tU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # Handle size mismatch\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                                    diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mjkaiZV0R7tU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet2D(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=6, init_features=32):\n",
        "        super().__init__()\n",
        "        features = init_features\n",
        "\n",
        "        self.inc = DoubleConv(in_channels, features)\n",
        "        self.down1 = Down(features, features * 2)\n",
        "        self.down2 = Down(features * 2, features * 4)\n",
        "        self.down3 = Down(features * 4, features * 8)\n",
        "        self.down4 = Down(features * 8, features * 16)\n",
        "\n",
        "        self.up1 = Up(features * 16, features * 8)\n",
        "        self.up2 = Up(features * 8, features * 4)\n",
        "        self.up3 = Up(features * 4, features * 2)\n",
        "        self.up4 = Up(features * 2, features)\n",
        "\n",
        "        self.outc = nn.Conv2d(features, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "trusted": true,
        "id": "JHZ1NOyAR7tU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize model"
      ],
      "metadata": {
        "id": "h2EG_62vR7tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = UNet2D(\n",
        "    in_channels=1,\n",
        "    out_channels=CONFIG['num_classes'],\n",
        "    init_features=CONFIG['init_features']\n",
        ").to(device)\n",
        "\n",
        "try:\n",
        "    summary(model, (1, CONFIG['spatial_size'][0], CONFIG['spatial_size'][1]))\n",
        "except:\n",
        "    print(\"Model summary not available, but model created successfully\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "J0iDqubAR7tU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "-iR8zShzR7tV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Trainer Class"
      ],
      "metadata": {
        "id": "n0Lz-eXTR7tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, config, device,\n",
        "                 checkpoint_dir='/kaggle/working', use_wandb=True):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.use_wandb = use_wandb\n",
        "\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Loss and optimizer\n",
        "        self.criterion = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='max', patience=5, factor=0.5, verbose=True\n",
        "        )\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction='mean')\n",
        "\n",
        "        # Training state\n",
        "        self.start_epoch = 0\n",
        "        self.best_dice = 0.0\n",
        "        self.history = []\n",
        "        self.wandb_run_id = None\n",
        "\n",
        "        # Paths\n",
        "        self.checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_2d.pth')\n",
        "        self.best_model_path = os.path.join(checkpoint_dir, 'best_model_2d.pth')\n",
        "        self.history_csv_path = os.path.join(checkpoint_dir, 'training_history_2d.csv')\n",
        "\n",
        "    def save_checkpoint(self, epoch, val_dice, is_best=False):\n",
        "        \"\"\"Save checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'best_dice': self.best_dice,\n",
        "            'history': self.history,\n",
        "            'config': self.config,\n",
        "            'wandb_run_id': self.wandb_run_id\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, self.checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "        if is_best:\n",
        "            torch.save(checkpoint, self.best_model_path)\n",
        "            print(f\"Best model saved! Dice: {val_dice:.4f}\")\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Load checkpoint if exists\"\"\"\n",
        "        if os.path.exists(self.checkpoint_path):\n",
        "            print(f\"Loading checkpoint from {self.checkpoint_path}\")\n",
        "            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            self.best_dice = checkpoint['best_dice']\n",
        "            self.history = checkpoint['history']\n",
        "\n",
        "            if 'wandb_run_id' in checkpoint:\n",
        "                self.wandb_run_id = checkpoint['wandb_run_id']\n",
        "\n",
        "            print(f\"Resumed from epoch {self.start_epoch}, best dice: {self.best_dice:.4f}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_history_csv(self):\n",
        "        \"\"\"Save training history to CSV\"\"\"\n",
        "        if self.history:\n",
        "            df = pd.DataFrame(self.history)\n",
        "            df.to_csv(self.history_csv_path, index=False)\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        \"\"\"Train one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
        "        for batch in pbar:\n",
        "            images = batch['image'].to(self.device)\n",
        "            segs = batch['seg'].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(images)\n",
        "            loss = self.criterion(outputs, segs)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def validate(self, epoch):\n",
        "        \"\"\"Validate\"\"\"\n",
        "        self.model.eval()\n",
        "        self.dice_metric.reset()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch} [Val]\")\n",
        "            for batch in pbar:\n",
        "                images = batch['image'].to(self.device)\n",
        "                segs = batch['seg'].to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                self.dice_metric(y_pred=outputs, y=segs)\n",
        "\n",
        "        val_dice = self.dice_metric.aggregate().item()\n",
        "        return val_dice\n",
        "\n",
        "    def train(self, num_epochs=None):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        if num_epochs is None:\n",
        "            num_epochs = self.config['num_epochs']\n",
        "\n",
        "        resumed = self.load_checkpoint()\n",
        "\n",
        "        if self.use_wandb:\n",
        "            if resumed and self.wandb_run_id:\n",
        "                wandb.init(\n",
        "                    project=\"my-2D-Unet-segment-RSNA\",\n",
        "                    config=self.config,\n",
        "                    resume=\"allow\",\n",
        "                    id=self.wandb_run_id,\n",
        "                    name=f\"unet2d_resumed_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "                )\n",
        "                print(f\"Resumed W&B run: {self.wandb_run_id}\")\n",
        "            else:\n",
        "                run_name = f\"unet2d_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "                wandb.init(\n",
        "                    project=\"my-2D-Unet-segment-RSNA\",\n",
        "                    config=self.config,\n",
        "                    name=run_name,\n",
        "                    tags=[\"2d-unet\", \"organ-segmentation\", \"rsna\"]\n",
        "                )\n",
        "                self.wandb_run_id = wandb.run.id\n",
        "                print(f\"Created new W&B run: {self.wandb_run_id}\")\n",
        "\n",
        "            wandb.watch(self.model, log='all', log_freq=100)\n",
        "\n",
        "        print(f\"\\nStarting training from epoch {self.start_epoch} to {num_epochs}\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()) / 1e6:.2f}M\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, num_epochs):\n",
        "            train_loss = self.train_epoch(epoch + 1)\n",
        "            val_dice = self.validate(epoch + 1)\n",
        "            self.scheduler.step(val_dice)\n",
        "\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            metrics = {\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': train_loss,\n",
        "                'val_dice': val_dice,\n",
        "                'learning_rate': current_lr,\n",
        "                'best_dice': self.best_dice\n",
        "            }\n",
        "\n",
        "            self.history.append(metrics)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"  Val Dice: {val_dice:.4f}\")\n",
        "            print(f\"  LR: {current_lr:.6f}\")\n",
        "            print(f\"  Best Dice: {self.best_dice:.4f}\")\n",
        "\n",
        "            if self.use_wandb:\n",
        "                wandb.log(metrics)\n",
        "\n",
        "            is_best = val_dice > self.best_dice\n",
        "            if is_best:\n",
        "                self.best_dice = val_dice\n",
        "\n",
        "            self.save_checkpoint(epoch, val_dice, is_best)\n",
        "            self.save_history_csv()\n",
        "\n",
        "        print(f\"\\nTraining complete! Best Dice: {self.best_dice:.4f}\")\n",
        "\n",
        "        if self.use_wandb:\n",
        "            wandb.finish()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hq4-_2dtR7tV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create trainer"
      ],
      "metadata": {
        "id": "KZd3eoB0R7tW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = UNetTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    config=CONFIG,\n",
        "    device=device,\n",
        "    checkpoint_dir=output_dir,\n",
        "    use_wandb=use_wandb\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "PzJkELTXR7tW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Start training"
      ],
      "metadata": {
        "id": "0QSEUhcfR7tW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean memory\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer.train(num_epochs=CONFIG['num_epochs'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "g5hA5S_9R7tW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "M7aTGxD7R7ti"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Load Best Model for Inference"
      ],
      "metadata": {
        "id": "PFUlUKV2R7tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_checkpoint = torch.load(trainer.best_model_path, map_location=device)\n",
        "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
        "print(f\"Best model loaded from epoch {best_checkpoint['epoch']} with Dice: {best_checkpoint['best_dice']:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ks-Q7CciR7tj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Evaluation & Visualization"
      ],
      "metadata": {
        "id": "umq0TGWMR7tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationEvaluator:\n",
        "    def __init__(self, model, val_loader, device, num_classes=6):\n",
        "        self.model = model\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.class_names = ['Background', 'Liver', 'Spleen', 'Kidney_L', 'Kidney_R', 'Bowel']\n",
        "\n",
        "    def compute_metrics(self):\n",
        "        \"\"\"Compute comprehensive metrics\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        class_metrics = {cls: {\n",
        "            'dice': [], 'iou': [], 'precision': [],\n",
        "            'recall': [], 'specificity': []\n",
        "        } for cls in range(1, self.num_classes)}\n",
        "\n",
        "        print(\"Computing metrics on validation set...\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader):\n",
        "                images = batch['image'].to(self.device)\n",
        "                labels = batch['seg'].to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                pred_np = preds.cpu().numpy()\n",
        "                label_np = labels.cpu().numpy()\n",
        "\n",
        "                for cls in range(1, self.num_classes):\n",
        "                    pred_cls = (pred_np == cls).astype(np.float32)\n",
        "                    label_cls = (label_np == cls).astype(np.float32)\n",
        "\n",
        "                    if label_cls.sum() == 0:\n",
        "                        continue\n",
        "\n",
        "                    intersection = (pred_cls * label_cls).sum()\n",
        "                    dice = (2 * intersection) / (pred_cls.sum() + label_cls.sum() + 1e-8)\n",
        "                    class_metrics[cls]['dice'].append(dice)\n",
        "\n",
        "                    union = pred_cls.sum() + label_cls.sum() - intersection\n",
        "                    iou = intersection / (union + 1e-8)\n",
        "                    class_metrics[cls]['iou'].append(iou)\n",
        "\n",
        "                    tp = intersection\n",
        "                    fp = pred_cls.sum() - intersection\n",
        "                    fn = label_cls.sum() - intersection\n",
        "                    tn = ((pred_cls == 0) & (label_cls == 0)).sum()\n",
        "\n",
        "                    precision = tp / (tp + fp + 1e-8)\n",
        "                    recall = tp / (tp + fn + 1e-8)\n",
        "                    specificity = tn / (tn + fp + 1e-8)\n",
        "\n",
        "                    class_metrics[cls]['precision'].append(precision)\n",
        "                    class_metrics[cls]['recall'].append(recall)\n",
        "                    class_metrics[cls]['specificity'].append(specificity)\n",
        "\n",
        "        results = {}\n",
        "        for cls in range(1, self.num_classes):\n",
        "            results[self.class_names[cls]] = {\n",
        "                metric: np.mean(values) if values else 0.0\n",
        "                for metric, values in class_metrics[cls].items()\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_metrics(self, results):\n",
        "        \"\"\"Plot metrics comparison\"\"\"\n",
        "        metrics_to_plot = ['dice', 'iou', 'precision', 'recall', 'specificity']\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for idx, metric in enumerate(metrics_to_plot):\n",
        "            classes = list(results.keys())\n",
        "            values = [results[cls][metric] for cls in classes]\n",
        "\n",
        "            axes[idx].bar(classes, values, color='steelblue', alpha=0.8)\n",
        "            axes[idx].set_title(f'{metric.upper()}', fontsize=14, fontweight='bold')\n",
        "            axes[idx].set_ylabel('Score', fontsize=12)\n",
        "            axes[idx].set_ylim([0, 1])\n",
        "            axes[idx].grid(axis='y', alpha=0.3)\n",
        "            axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "            for i, v in enumerate(values):\n",
        "                axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
        "\n",
        "        axes[5].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'metrics_comparison_2d.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"Metrics plot saved to {output_dir}/metrics_comparison_2d.png\")\n",
        "\n",
        "    def visualize_predictions(self, num_samples=5):\n",
        "        \"\"\"Visualize overlay masks for 2D slices\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        colors = ['black', 'red', 'green', 'blue', 'yellow', 'purple']\n",
        "        cmap = ListedColormap(colors)\n",
        "\n",
        "        samples_shown = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                if samples_shown >= num_samples:\n",
        "                    break\n",
        "\n",
        "                images = batch['image'].to(self.device)\n",
        "                labels = batch['seg'].to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                img_np = images[0, 0].cpu().numpy()\n",
        "                pred_np = preds[0].cpu().numpy()\n",
        "                label_np = labels[0, 0].cpu().numpy()\n",
        "\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "                # Original image\n",
        "                axes[0].imshow(img_np, cmap='gray')\n",
        "                axes[0].set_title('CT Image', fontsize=14, fontweight='bold')\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                # Ground truth\n",
        "                axes[1].imshow(img_np, cmap='gray')\n",
        "                axes[1].imshow(label_np, cmap=cmap, alpha=0.5, vmin=0, vmax=5)\n",
        "                axes[1].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
        "                axes[1].axis('off')\n",
        "\n",
        "                # Prediction\n",
        "                axes[2].imshow(img_np, cmap='gray')\n",
        "                axes[2].imshow(pred_np, cmap=cmap, alpha=0.5, vmin=0, vmax=5)\n",
        "                axes[2].set_title('Prediction', fontsize=14, fontweight='bold')\n",
        "                axes[2].axis('off')\n",
        "\n",
        "                # Add legend\n",
        "                from matplotlib.patches import Patch\n",
        "                legend_elements = [Patch(facecolor=colors[i], label=self.class_names[i])\n",
        "                                 for i in range(1, len(colors))]\n",
        "                fig.legend(handles=legend_elements, loc='lower center', ncol=5, fontsize=12)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, f'overlay_2d_sample_{samples_shown+1}.png'),\n",
        "                           dpi=150, bbox_inches='tight')\n",
        "                plt.show()\n",
        "\n",
        "                samples_shown += 1\n",
        "\n",
        "        print(f\"Overlay visualizations saved to {output_dir}/overlay_2d_sample_*.png\")\n",
        "\n",
        "    def plot_per_class_comparison(self, num_samples=3):\n",
        "        \"\"\"Plot detailed per-class segmentation comparison\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        colors = ['black', 'red', 'green', 'blue', 'yellow', 'purple']\n",
        "\n",
        "        samples_shown = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                if samples_shown >= num_samples:\n",
        "                    break\n",
        "\n",
        "                images = batch['image'].to(self.device)\n",
        "                labels = batch['seg'].to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                img_np = images[0, 0].cpu().numpy()\n",
        "                pred_np = preds[0].cpu().numpy()\n",
        "                label_np = labels[0, 0].cpu().numpy()\n",
        "\n",
        "                # Create figure with per-organ comparison\n",
        "                fig, axes = plt.subplots(2, self.num_classes, figsize=(20, 8))\n",
        "\n",
        "                for cls_idx in range(self.num_classes):\n",
        "                    # Ground truth for this class\n",
        "                    gt_mask = (label_np == cls_idx).astype(float)\n",
        "                    axes[0, cls_idx].imshow(img_np, cmap='gray')\n",
        "                    axes[0, cls_idx].imshow(gt_mask, cmap='Reds', alpha=0.5, vmin=0, vmax=1)\n",
        "                    axes[0, cls_idx].set_title(f'{self.class_names[cls_idx]}\\nGround Truth',\n",
        "                                               fontsize=10, fontweight='bold')\n",
        "                    axes[0, cls_idx].axis('off')\n",
        "\n",
        "                    # Prediction for this class\n",
        "                    pred_mask = (pred_np == cls_idx).astype(float)\n",
        "                    axes[1, cls_idx].imshow(img_np, cmap='gray')\n",
        "                    axes[1, cls_idx].imshow(pred_mask, cmap='Blues', alpha=0.5, vmin=0, vmax=1)\n",
        "                    axes[1, cls_idx].set_title(f'{self.class_names[cls_idx]}\\nPrediction',\n",
        "                                               fontsize=10, fontweight='bold')\n",
        "                    axes[1, cls_idx].axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, f'per_class_comparison_{samples_shown+1}.png'),\n",
        "                           dpi=150, bbox_inches='tight')\n",
        "                plt.show()\n",
        "\n",
        "                samples_shown += 1\n",
        "\n",
        "        print(f\"Per-class comparison saved to {output_dir}/per_class_comparison_*.png\")\n",
        "\n",
        "    def plot_confusion_analysis(self):\n",
        "        \"\"\"Plot confusion matrix for segmentation classes\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        print(\"Computing confusion matrix...\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader):\n",
        "                images = batch['image'].to(self.device)\n",
        "                labels = batch['seg'].to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                all_preds.append(preds.cpu().numpy().flatten())\n",
        "                all_labels.append(labels.cpu().numpy().flatten())\n",
        "\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds, labels=range(self.num_classes))\n",
        "\n",
        "        # Normalize\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        # Plot\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # Raw counts\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=self.class_names, yticklabels=self.class_names,\n",
        "                    ax=ax1, cbar_kws={'label': 'Count'})\n",
        "        ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "        ax1.set_ylabel('True Label', fontsize=12)\n",
        "        ax1.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "        # Normalized\n",
        "        sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Greens',\n",
        "                    xticklabels=self.class_names, yticklabels=self.class_names,\n",
        "                    ax=ax2, cbar_kws={'label': 'Proportion'})\n",
        "        ax2.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "        ax2.set_ylabel('True Label', fontsize=12)\n",
        "        ax2.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'confusion_matrix_2d.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"Confusion matrix saved to {output_dir}/confusion_matrix_2d.png\")\n",
        "\n",
        "    def create_summary_report(self, results):\n",
        "        \"\"\"Create text summary report\"\"\"\n",
        "        report_path = os.path.join(output_dir, 'evaluation_report_2d.txt')\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(\"2D SEGMENTATION EVALUATION REPORT\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for cls_name, metrics in results.items():\n",
        "                f.write(f\"\\n{cls_name}:\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\")\n",
        "                f.write(f\"  Dice Coefficient:  {metrics['dice']:.4f}\\n\")\n",
        "                f.write(f\"  IoU:              {metrics['iou']:.4f}\\n\")\n",
        "                f.write(f\"  Precision:        {metrics['precision']:.4f}\\n\")\n",
        "                f.write(f\"  Recall:           {metrics['recall']:.4f}\\n\")\n",
        "                f.write(f\"  Specificity:      {metrics['specificity']:.4f}\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "            f.write(\"OVERALL AVERAGES:\\n\")\n",
        "            f.write(\"-\"*60 + \"\\n\")\n",
        "\n",
        "            for metric in ['dice', 'iou', 'precision', 'recall', 'specificity']:\n",
        "                avg = np.mean([results[cls][metric] for cls in results.keys()])\n",
        "                f.write(f\"  Average {metric.upper()}: {avg:.4f}\\n\")\n",
        "\n",
        "        print(f\"\\nEvaluation report saved to {report_path}\")\n",
        "\n",
        "        with open(report_path, 'r') as f:\n",
        "            print(f.read())"
      ],
      "metadata": {
        "trusted": true,
        "id": "DjNS-qOAR7tj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = SegmentationEvaluator(model, val_loader, device, num_classes=CONFIG['num_classes'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "wQKcWpoyPD9-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Compute all metrics"
      ],
      "metadata": {
        "id": "SK8YYZXGR7tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing metrics...\")\n",
        "results = evaluator.compute_metrics()"
      ],
      "metadata": {
        "trusted": true,
        "id": "zW2rFwVHR7tk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Plot metrics comparison"
      ],
      "metadata": {
        "id": "TcXe_d7IR7tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Plotting metrics comparison\")\n",
        "evaluator.plot_metrics(results)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VeBHYbVnR7tk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Visualize overlay masks"
      ],
      "metadata": {
        "id": "tvWmLvGSR7tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating overlay visualizations...\")\n",
        "evaluator.visualize_predictions(num_samples=5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4ob1BrgCR7tl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Per-class comparison"
      ],
      "metadata": {
        "id": "7bYpgIveR7tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating per-class comparison...\")\n",
        "evaluator.plot_per_class_comparison(num_samples=3)"
      ],
      "metadata": {
        "trusted": true,
        "id": "N_G3gPKFR7tl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Confusion matrix analysis"
      ],
      "metadata": {
        "id": "zOZFQgM5PD-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating confusion matrix...\")\n",
        "evaluator.plot_confusion_analysis()"
      ],
      "metadata": {
        "trusted": true,
        "id": "L9MKTMZyR7tl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create summary report"
      ],
      "metadata": {
        "id": "gYTMReldR7tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.create_summary_report(results)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-CZdOjUOR7tl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "wU6QwUEsR7tl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}